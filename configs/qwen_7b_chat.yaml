# Model Configuration
model:
  name_or_path: "/home/models/Qwen/Qwen2.5-7B"
  max_length: 2048
  temperature: 0.7
  top_p: 0.9
  do_sample: true

# LoRA Configuration
lora:
  r: 16
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

# Training Configuration
training:
  num_epochs: 3
  batch_size: 4
  learning_rate: 5e-5
  warmup_steps: 100
  max_grad_norm: 1.0
  save_steps: 500
  eval_steps: 100

# HGA Configuration
hga:
  max_turns: 5
  utility_threshold: 0.5
  attacker_loss_weight: 0.3
  safety_weight: 1.0
  uselessness_weight: 0.2
  mcts_simulations: 50
  
# Data Configuration
data:
  datasets: ["SafeNLP/ActorAttack_test", "PKU-Alignment/BeaverTails"]
  max_samples: 1000
  train_split: 0.8

# Evaluation Configuration
evaluation:
  asr_datasets: ["SafeNLP/ActorAttack_test"]
  general_datasets: ["cais/mmlu", "gsm8k"]
  batch_size: 8